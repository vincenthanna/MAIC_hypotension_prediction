{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>body {font-family: \"D2Coding\", cursive, sans-serif;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "display(HTML(\"<style>body {font-family: \\\"D2Coding\\\", cursive, sans-serif;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GMtwtoCVfJSL"
   },
   "outputs": [],
   "source": [
    "#import keras\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, BatchNormalization, Dropout, Activation, Add, Layer, GlobalAveragePooling1D, Input, Concatenate,Reshape, Dense, multiply, add, Permute, Lambda\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "c1vtbSYQfkxX",
    "outputId": "6c4c27fc-4bc2-4621-85cb-e3a6966daf3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseid</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>M</td>\n",
       "      <td>67.50</td>\n",
       "      <td>160.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>M</td>\n",
       "      <td>53.00</td>\n",
       "      <td>160.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>52</td>\n",
       "      <td>F</td>\n",
       "      <td>62.30</td>\n",
       "      <td>167.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>72</td>\n",
       "      <td>M</td>\n",
       "      <td>62.75</td>\n",
       "      <td>162.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>F</td>\n",
       "      <td>64.90</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>57</td>\n",
       "      <td>M</td>\n",
       "      <td>68.90</td>\n",
       "      <td>162.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17</td>\n",
       "      <td>85</td>\n",
       "      <td>M</td>\n",
       "      <td>53.00</td>\n",
       "      <td>164.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19</td>\n",
       "      <td>74</td>\n",
       "      <td>M</td>\n",
       "      <td>66.20</td>\n",
       "      <td>171.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>75</td>\n",
       "      <td>M</td>\n",
       "      <td>61.30</td>\n",
       "      <td>173.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>54.60</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   caseid  age sex  weight  height\n",
       "0       1   77   M   67.50   160.2\n",
       "1       4   74   M   53.00   160.6\n",
       "2       7   52   F   62.30   167.7\n",
       "3      10   72   M   62.75   162.5\n",
       "4      13   67   F   64.90   153.0\n",
       "5      16   57   M   68.90   162.3\n",
       "6      17   85   M   53.00   164.2\n",
       "7      19   74   M   66.20   171.3\n",
       "8      20   75   M   61.30   173.6\n",
       "9      22   53   F   54.60   162.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = \"./\"\n",
    "FILE_TRAIN_CASES = \"./train_cases.csv\"    \n",
    "    \n",
    "df_tc = pd.read_csv(FILE_TRAIN_CASES)\n",
    "df_tc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "J8kYAWr3gJkw",
    "outputId": "5808471d-52c1-4756-de06-045cf894fe51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[ 4.5  5.5  6.5  7.5  8.5  9.5 10.5 11.5 12.5 13.5 14.5]\n"
     ]
    }
   ],
   "source": [
    "def moving_average(a, n=200):\n",
    "    '''\n",
    "    각각 size=n 만큼의 윈도우의 평균값을 갖게한다.\n",
    "    '''\n",
    "    ret = np.nancumsum(a, dtype=np.float32) # 누적합 배열을 만든다.        \n",
    "    '''\n",
    "    ret[:-n] 처음부터 시작, 끝에서 n개 뺀 것까지. (0 ~ len(arr)-n)\n",
    "    \n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    assume n = 5 :\n",
    "      5 6 7 8 9 10 11 12 13 ...\n",
    "    - 0 1 2 3 4  5  6  7  8 ...\n",
    "    ------------------------\n",
    "      5 5 5 5 5  5  5 ....\n",
    "\n",
    "    여기서 ret가 누적합이므로, moving average값이 된다.\n",
    "    '''\n",
    "    ret[n:] = ret[n:] - ret[:-n] # window에 포함된 값들의 총합들을 구한다. n개의 합부터 시작\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "a = [i for i in range(20)]\n",
    "print(a)\n",
    "print(moving_average(a, n=10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "DATA_VERSION = 14\n",
    "\n",
    "sample_rate = 100\n",
    "\n",
    "MINUTES_AHEAD = 5\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "\n",
    "def make_filename(typename, ext):\n",
    "    ret = typename + '_' + str(DATA_VERSION) + \"_\" + str(random_keyval) + '.' + ext\n",
    "    ret = os.path.join(BASE_DIR, ret)\n",
    "    return ret\n",
    "\n",
    "\n",
    "# As provided in the answer by Divakar\n",
    "def ffill(arr):\n",
    "    mask = np.isnan(arr)\n",
    "    idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n",
    "    np.maximum.accumulate(idx, axis=1, out=idx)\n",
    "    out = arr[np.arange(idx.shape[0])[:,None], idx]\n",
    "    return out\n",
    "\n",
    "\n",
    "# My modification to do a backward-fill\n",
    "def bfill(arr):\n",
    "    mask = np.isnan(arr)\n",
    "    idx = np.where(~mask, np.arange(mask.shape[1]), mask.shape[1] - 1)\n",
    "    idx = np.minimum.accumulate(idx[:, ::-1], axis=1)[:, ::-1]\n",
    "    out = arr[np.arange(idx.shape[0])[:,None], idx]\n",
    "    return out\n",
    "\n",
    "\n",
    "def fb_fill_1dim(arr):\n",
    "    arr = arr.reshape((1, -1))\n",
    "    arr = ffill(arr)\n",
    "    arr = bfill(arr)\n",
    "    arr = arr.reshape((-1))\n",
    "    return arr\n",
    "\n",
    "def min_to_sec(minval):\n",
    "    return minval * 60\n",
    "\n",
    "def to_sr(a):\n",
    "    return sample_rate * a\n",
    "\n",
    "in_wlen = to_sr(20)\n",
    "\n",
    "def calc_hb_hr(segx):\n",
    "    \"\"\"extrace number of heartbeats in segment and heartbeat rate\"\"\"\n",
    "    \n",
    "    prominence_candidates = [20, 16, 12, 10]\n",
    "    for prominence in prominence_candidates:\n",
    "        peaks, _ = find_peaks(segx, distance=50, prominence=prominence)    \n",
    "        heartbeats = len(peaks)\n",
    "        diffs = np.diff(peaks)\n",
    "        if len(diffs) == 0 or heartbeats == 0:\n",
    "            continue            \n",
    "        heartrates = round(in_wlen / np.diff(peaks).mean())\n",
    "        return heartbeats, heartrates\n",
    "    return 0, 0\n",
    "\n",
    "\n",
    "def check_valid(segx):    \n",
    "    if np.mean(np.isnan(segx)) > 0.1 or \\\n",
    "        np.max(segx) > 200 or np.min(segx) < 20 or \\\n",
    "        np.max(segx) - np.min(segx) < 30 or \\\n",
    "        (np.abs(np.diff(segx[~np.isnan(segx)])) > 30).any():\n",
    "        return False\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102281, 5)\n",
      "[0.77631579 0.33882784 0.46007605 0.         1.        ]\n",
      "(102281, 5)\n"
     ]
    }
   ],
   "source": [
    "# extract demographic data from test dataset\n",
    "if os.path.exists(\"x_test_cases.csv\") == False:\n",
    "    x_test = pd.read_csv(os.path.join(BASE_DIR, 'test2_x.csv'))    \n",
    "    x_test_cases = x_test[['age', 'sex', 'weight', 'height']]\n",
    "    x_test_cases.to_csv(\"x_test_cases.csv\", index=False)\n",
    "\n",
    "    \n",
    "def get_scaler():\n",
    "    \"\"\"generator에서 데이터 생성할 때마다 사용할 scaler를 생성함\"\"\"\n",
    "    \n",
    "    x_train_cases = pd.read_csv(FILE_TRAIN_CASES)\n",
    "    x_train_cases = x_train_cases.drop('caseid', axis=1)\n",
    "    x_test_cases = pd.read_csv(\"x_test_cases.csv\")\n",
    "    tmp = pd.concat([x_train_cases, x_test_cases])\n",
    "    train_len = x_train_cases.shape[0]\n",
    "    tmp = pd.get_dummies(tmp)\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    print(tmp.shape)\n",
    "    tmp = scaler.fit_transform(tmp)    \n",
    "    print(tmp[0])\n",
    "    print(tmp.shape)\n",
    "    return scaler\n",
    "\n",
    "\n",
    "demographic_info_scaler = get_scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "90hoM0y0lHVk",
    "outputId": "925defa9-6b3a-48bc-eec8-283655eaff36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 1-epoch Data 100%\n"
     ]
    }
   ],
   "source": [
    "def build_one_epoch(scaler, frac=0.5):\n",
    "    gc.collect()\n",
    "\n",
    "    x_train = []\n",
    "    x_train_cases = []\n",
    "    y_train = []\n",
    "    info_columns = ['caseid', 'age', 'sex', 'weight', 'height']\n",
    "    hb_err_cnt = 0\n",
    "\n",
    "    df_tc = pd.read_csv(FILE_TRAIN_CASES)    \n",
    "    df_tc = df_tc.sample(frac=frac)\n",
    "        \n",
    "    num_processed = 0\n",
    "    for idx, row in df_tc.iterrows():\n",
    "        caseid = row['caseid']\n",
    "        age = row['age']\n",
    "        sex = row['sex']\n",
    "        weight = row['weight']\n",
    "        height = row['height']\n",
    "        \n",
    "        num_processed += 1\n",
    "        \n",
    "        b = \"Building 1-epoch Data \" + str(int((num_processed * 100)/df_tc.shape[0])) + \"%\"\n",
    "        sys.stdout.write('\\r'+ b)\n",
    "\n",
    "        fpath = os.path.join(BASE_DIR, ''.join(['train_data/', str(caseid), '.csv']))\n",
    "        if os.path.isfile(fpath):\n",
    "            samples = pd.read_csv(fpath, header=None).values.flatten() # 읽은값이 (x, 1) 이어서 1차원으로 바꿔준다.\n",
    "\n",
    "            # 20sec (20 00) - 5min (300 00) - 1min (60 00) = 38000 sample\n",
    "            \n",
    "            i = random.randint(to_sr(10), to_sr(40)) # random start\n",
    "\n",
    "            event_idx = []\n",
    "            non_event_idx = []\n",
    "\n",
    "            yidx_start = to_sr(20 + min_to_sec(MINUTES_AHEAD)) # output 크기\n",
    "            yidx_end = yidx_start + to_sr(min_to_sec(1))\n",
    "            while i < (len(samples) - yidx_end) :                \n",
    "                segx = samples[i : i + in_wlen] # 20sec samples window\n",
    "                segy = samples[i + yidx_start : i + yidx_end] # 출력 : 20초 후부터 5분 후 1분동안의 MAP값\n",
    "                \n",
    "                if (check_valid(segx) == False) or (check_valid(segy) == False):\n",
    "                    i += random.randint(to_sr(1), to_sr(10)) # random move\n",
    "                    continue\n",
    "\n",
    "                segx = fb_fill_1dim(segx) # nan 값을 앞뒤 값으로 채운다.\n",
    "\n",
    "                # demographic data 패키징\n",
    "                info_pack = [caseid, age, sex, weight, height]\n",
    "\n",
    "                # 출력변수\n",
    "                segy = moving_average(segy, to_sr(2)) # 2 sec moving average\n",
    "\n",
    "                # event 여부 확인\n",
    "                event = 1 if np.nanmax(segy) < 65 else 0\n",
    "\n",
    "                # train 데이터 추가\n",
    "                event_idx.append(i) if event else non_event_idx.append(i)\n",
    "                x_train.append(segx)\n",
    "                y_train.append(event)\n",
    "                x_train_cases.append(info_pack)\n",
    "                \n",
    "                # move i by random\n",
    "                i += random.randint(to_sr(5), to_sr(35))\n",
    "\n",
    "            nsamp = len(event_idx) + len(non_event_idx)\n",
    "\n",
    "    print()\n",
    "    x_train = np.array(x_train, dtype=np.float32)\n",
    "    \n",
    "    # postprocess x_train value if needed\n",
    "    x_train -= 65\n",
    "    x_train /= 65\n",
    "\n",
    "    y_train = np.array(y_train, dtype=np.float32)\n",
    "    \n",
    "    # one-hot encode categorical columns and normalize.\n",
    "    x_train_cases = pd.DataFrame(x_train_cases, columns = info_columns)\n",
    "    gc.collect()\n",
    "    _xtrain_cases = x_train_cases\n",
    "    if 'caseid' in _xtrain_cases:\n",
    "        _xtrain_cases = _xtrain_cases.drop('caseid', axis=1)\n",
    "    _xtest_cases = pd.read_csv(\"x_test_cases.csv\")\n",
    "    tmp = pd.concat([_xtrain_cases, _xtest_cases])\n",
    "    _xtrain_cases_len = x_train_cases.shape[0]\n",
    "    tmp = pd.get_dummies(tmp)\n",
    "    tmp = scaler.transform(tmp)    \n",
    "    xtrain_cases = tmp[:_xtrain_cases_len]    \n",
    "\n",
    "    # shuffle data several times\n",
    "    shuffle_cnt = random.randint(4, 10)\n",
    "    for i in range(shuffle_cnt):\n",
    "        s = np.arange(x_train.shape[0])\n",
    "        np.random.shuffle(s)\n",
    "        x_train = x_train[s]\n",
    "        y_train = y_train[s]\n",
    "        xtrain_cases = xtrain_cases[s]\n",
    "    \n",
    "    # concatenate MBP and demographic data\n",
    "    # will be split in first layer of model\n",
    "    x_train = np.concatenate([x_train, xtrain_cases], axis=1)    \n",
    "    return x_train, y_train   \n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    build_one_epoch(scaler=demographic_info_scaler, frac=0.01)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.utils.Sequence):\n",
    "    # Class is a dataset wrapper for better training performance\n",
    "    def __init__(self, batch_size=256, frac=0.05):\n",
    "        self.batch_size = batch_size\n",
    "        self.frac = frac\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self): # return number of batches\n",
    "        #print(self.x_train.shape[0], self.batch_size)\n",
    "        num_batch = int(self.x_train.shape[0] / self.batch_size)\n",
    "        if self.x_train.shape[0] % self.batch_size > 0:\n",
    "            num_batch += 1\n",
    "        #print(\"num batch=\", num_batch)\n",
    "        return num_batch\n",
    "\n",
    "    def __getitem__(self, idx): # return 1-batch data\n",
    "        start = idx * self.batch_size\n",
    "        end = (idx+1) * self.batch_size\n",
    "        if end > len(self.x_train):\n",
    "            end = len(self.x_train)\n",
    "        \n",
    "        batch_x = self.x_train[start:end]\n",
    "        batch_y = self.y_train[start:end]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        x_train, y_train = build_one_epoch(scaler=demographic_info_scaler, frac=self.frac)\n",
    "        self.x_train = x_train        \n",
    "        self.y_train = y_train\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape= (None, 2000, 1)\n",
      "x.shape= (None, 1000, 32)\n",
      "x.shape= (None, 500, 32)\n",
      "x.shape= (None, 250, 32)\n",
      "x.shape= (None, 125, 64)\n",
      "x.shape= (None, 63, 64)\n",
      "x.shape= (None, 32, 64)\n",
      "x.shape= (None, 16, 128)\n",
      "x.shape= (None, 133, 1)\n",
      "x.shape= (None, 67, 16)\n",
      "x.shape= (None, 34, 32)\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "def squeeze_excite_block(input_tensor, ratio=16):\n",
    "    \"\"\" Create a channel-wise squeeze-excite block\n",
    "    Args:\n",
    "        input_tensor: input Keras tensor\n",
    "        ratio: number of output filters\n",
    "    Returns: a Keras tensor\n",
    "    References\n",
    "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
    "    \"\"\"\n",
    "    init = input_tensor\n",
    "    channel_axis = 1 if tf.keras.backend.image_data_format() == \"channels_first\" else -1\n",
    "    filters = init.shape[channel_axis]\n",
    "    se_shape = (1, filters)\n",
    "\n",
    "    se = keras.layers.GlobalAveragePooling1D()(init)\n",
    "    se = Reshape(se_shape)(se)    \n",
    "    se = keras.layers.Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = keras.layers.Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)    \n",
    "\n",
    "    x = multiply([init, se])\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_basic_block(inputs, filters, kernel_size, strides):\n",
    "    x = inputs\n",
    "    shortcut = inputs\n",
    "    print(\"x.shape=\", x.shape)\n",
    "    x = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding='same', activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)    \n",
    "\n",
    "    x = squeeze_excite_block(x)\n",
    "    \n",
    "    shortcut = keras.layers.Conv1D(filters=filters, kernel_size=1, strides=strides, padding='same', activation='relu')(shortcut)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = keras.layers.Add()([shortcut, x])\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet(inputs, kernel_size, filter_sizes):\n",
    "    x = inputs\n",
    "    for fsize in filter_sizes:\n",
    "        x = resnet_basic_block(inputs=x, filters=fsize, kernel_size=kernel_size, strides=2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_demo_net(inputs, kernel_sizes, filter_sizes):\n",
    "    \n",
    "    # you have to use keras layer class for splitting.\n",
    "    # just splitting like array doesn't work    \n",
    "    \n",
    "    # bp data\n",
    "    input_bp = Lambda(lambda x: x[:, :2000])(inputs)\n",
    "    input_bp = tf.keras.backend.expand_dims(input_bp, axis=-1)\n",
    "    \n",
    "    # demographic\n",
    "    input_demo = Lambda(lambda x: x[:, 2000:])(inputs)\n",
    "\n",
    "    sub_models = []\n",
    "    for kernel_size in kernel_sizes:        \n",
    "        sm = resnet(input_bp, kernel_size=kernel_size, filter_sizes=filter_sizes)\n",
    "        sm = keras.layers.GlobalAveragePooling1D()(sm)        \n",
    "        sub_models.append(sm)\n",
    "\n",
    "    if len(sub_models) > 1:\n",
    "        #x = keras.layers.Add()(sub_models)\n",
    "        x = keras.layers.Concatenate()(sub_models)\n",
    "    else:\n",
    "        x = sub_models[0]\n",
    "    \n",
    "    x = Concatenate()([x, input_demo])\n",
    "\n",
    "    x = tf.expand_dims(x, -1)\n",
    "\n",
    "    x = resnet_basic_block(x, filters=16, kernel_size=3, strides=2)\n",
    "    x = resnet_basic_block(x, filters=32, kernel_size=3, strides=2)\n",
    "    x = resnet_basic_block(x, filters=64, kernel_size=3, strides=2)\n",
    "    \n",
    "    x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    x = keras.layers.Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "input_tensor = Input(shape=(2005,), dtype='float32')\n",
    "\n",
    "kernel_sizes = [3]\n",
    "filter_sizes = [32, 32, 32, 64, 64, 64, 128, 128]\n",
    "\n",
    "def prepare_model():\n",
    "    tf.keras.backend.clear_session()\n",
    "    mergenet = resnet_demo_net(input_tensor, kernel_sizes=kernel_sizes, filter_sizes=filter_sizes)\n",
    "    mergenet_model = keras.models.Model(inputs=input_tensor, outputs=mergenet)\n",
    "    mergenet_model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(), metrics=['accuracy', tf.keras.metrics.AUC(name=\"auc_mergenet\")])\n",
    "    return mergenet_model\n",
    "\n",
    "mergenet_model = prepare_model()\n",
    "plot_model(mergenet_model, to_file='lowprenet.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 1-epoch Data 100%\n",
      "Building 1-epoch Data 100%\n",
      "Building 1-epoch Data 100%===========] - ETA: 0s - loss: 0.3811 - accuracy: 0.9293 - auc_mergenet: 0.8254\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.38113, saving model to ./se_resnet_deep_end_resnet_generator_dataset_6\\mergenet_modelweights.hdf5\n",
      "33/33 [==============================] - 90s 3s/step - loss: 0.3811 - accuracy: 0.9293 - auc_mergenet: 0.8254 - val_loss: 1024453443584.0000 - val_accuracy: 0.9486 - val_auc_mergenet: 0.8676\n",
      "Building 1-epoch Data 100%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "def make_callbacks(weight_path):\n",
    "    callbacks = []    \n",
    "    callbacks.append(ModelCheckpoint(monitor='loss', filepath=weight_path, verbose=1, save_best_only=True))\n",
    "    #callbacks.append(ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience=2, min_lr=0.00001, verbose=1, mode='min'))    \n",
    "    callbacks.append(EarlyStopping(monitor='val_auc_mergenet', patience=1, verbose=0, mode='max', min_delta=0.015))\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "def train_model(model, weight_path, saving_model_path, batch_size, epochs):    \n",
    "\n",
    "    callbacks = make_callbacks(weight_path)\n",
    "    \n",
    "    train_gen = Generator(BATCH_SIZE, frac=0.01)\n",
    "    valid_gen = Generator(BATCH_SIZE, frac=0.01)\n",
    "    \n",
    "    hist = model.fit(train_gen, validation_data=valid_gen, batch_size=batch_size, class_weight={0:1, 1:10}, callbacks=callbacks, epochs=epochs)\n",
    "    \n",
    "    open(saving_model_path, \"wt\").write(model.to_json())   \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# # train mbp-only model\n",
    "model_ver = 6\n",
    "model_name = \"mergenet_model\"\n",
    "outdir = \"se_resnet_deep_end_resnet_generator_dataset\" + \"_\" + str(model_ver)\n",
    "weight_path = os.path.join(BASE_DIR, outdir, model_name + \"weights.hdf5\")\n",
    "os.makedirs(os.path.dirname(weight_path), exist_ok=True)\n",
    "saving_model_path = os.path.join(BASE_DIR, outdir, model_name + \"model.json\")\n",
    "\n",
    "\n",
    "model = train_model(mergenet_model, weight_path, saving_model_path, BATCH_SIZE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # prepare test data  \n",
    "    x_test = []\n",
    "    x_test_cases = None\n",
    "    \n",
    "    x_test = pd.read_csv(os.path.join(BASE_DIR, 'test2_x.csv'))    \n",
    "    x_test = x_test.values\n",
    "    x_test = np.array(x_test[:, 4:], dtype=np.float32) # skip demographic values(age, sex weight, height)\n",
    "    x_test = ffill(x_test)\n",
    "    x_test = bfill(x_test)\n",
    "    x_test -= 65\n",
    "    x_test /= 65\n",
    "    \n",
    "    x_test_cases = pd.read_csv(\"x_test_cases.csv\")\n",
    "    x_test_cases = pd.get_dummies(x_test_cases)\n",
    "    xtest_cases = demographic_info_scaler.transform(x_test_cases)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_input = np.concatenate([x_test, xtest_cases], axis=1)\n",
    "\n",
    "y_pred = model.predict(x = x_test_input).flatten()\n",
    "\n",
    "def savepredfile(filename, pred):\n",
    "    savefilename = os.path.join(BASE_DIR, \"preds\")\n",
    "    savefilename = os.path.join(savefilename, filename)\n",
    "    np.savetxt(savefilename, pred, fmt=\"%.4f\")\n",
    "    np.savetxt(filename, pred, fmt=\"%.4f\")\n",
    "\n",
    "savepredfile(\"pred_y.txt\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MAIC Data Builder",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
